{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import io\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from __future__ import print_function\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import hstack\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print dataset rows\n",
    "def print_str_cells(df, col_name, n_lines, hide_special_chars=False):\n",
    "    for i in range(0,n_lines+1):\n",
    "        print(\"LINE NUMBER: {}\".format(i))\n",
    "        if hide_special_chars:\n",
    "            print(df.loc[i, col_name])\n",
    "        else:\n",
    "            print(repr(df.loc[i, col_name]))\n",
    "        print(\"================== \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load File\n",
    "df = pd.read_csv('./dataset_II_pre_process_II.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Relevante.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print rows of the (title)\n",
    "print_str_cells(df, \"title\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print rows of the (content)\n",
    "print_str_cells(df, \"content\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Relevante'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing the class distribution of the dataset\n",
    "rel_dist = df.groupby('Relevante').size().reset_index()\n",
    "rel_dist = rel_dist.rename(columns={0: 'posts'})\n",
    "rel_dist.plot(kind='barh', x=\"Relevante\", y=\"posts\", figsize=(10,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# In this case, data cleaning has already been done in pre_process_II\n",
    "# The cleaned data is in the column df['full_text']\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "###############\n",
    "\n",
    "df['title'] = df['title'].astype(str)\n",
    "df['content'] = df['content'].astype(str)\n",
    "df['answers'] = df['answers'].astype(str)\n",
    "\n",
    "full_text_col = 'full_text'\n",
    "required_columns = [\"title\", \"content\", \"answers\", full_text_col]\n",
    "df[full_text_col] = df[\"title\"] + \" \" + df[\"content\"] + df[\"answers\"]\n",
    "\n",
    "###############\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "stop_words.update([\"pra\",\"etc\", \"none\", \"vai\", \"ter\", \"nan\", \"user\", \"author\", \"title\", \"none\",\n",
    "                    \"name\", \"score\", \"content\", \"down\", \"votes\", \"created\", \"comments\", \"comment\",\n",
    "                    \"answercontent\", \"vote\", \"type\", \"points\", \"aqui\", \"pode\", \"sobre\", \"fazer\",\n",
    "                    \"alguem\", \"tudo\", \"regular\", \"coisa\", \"bem\", \"vou\", \"sei\", \"boca\", \"algum\",\n",
    "                    \"alguns\", \"alguma\", \"algo\", \"nada\", \"bom\", \"entao\", \"acho\", \"quer\", \"the\",\n",
    "                    \"and\", \"you\", \"cara\", \"coisas\", \"sim\", \"ainda\", \"ver\", \"usar\", \"assim\",\n",
    "                    \"index\"])\n",
    "\n",
    "# Replace characters with accents with their unaccented equivalents\n",
    "stop_words_without_accents = set()\n",
    "for word in stop_words:\n",
    "    stop_words_without_accents.add(unidecode(word))\n",
    "stop_words = stop_words_without_accents\n",
    "\n",
    "def preprocess_text(text):    \n",
    "    # Replace characters with accents with their unaccented equivalents\n",
    "    text = unidecode(text)\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    # Replace \\\\n with a white space\n",
    "    text = re.sub(r'\\s*\\\\n\\s*', ' ', text)\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    # Remove .onion links\n",
    "    text = re.sub(r'\\S*\\.onion\\S*', ' ', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    # Remove the titles that appear in answers\n",
    "    text = re.sub(r\"'title': [^,]*,\", ' ', text)\n",
    "    # Remove the usernames that appear in answers\n",
    "    text = re.sub(r\"'name': [^,]*,\", ' ', text)\n",
    "    # Remove the types that appear in answers \n",
    "    text = re.sub(r\"'type': [^,]*,\", ' ', text)\n",
    "    # Remove the authors that appear in answers \n",
    "    text = re.sub(r\"'author': [^,]*,\", ' ', text)\n",
    "    # Remove sequences of kkkk\n",
    "    text = re.sub(r'k{2,}\\S*', ' ', text)\n",
    "    \n",
    "    # Remove terms with more than 4 consecutive consonants\n",
    "    consonants_5m = \"([bcdfghjklmnpqrstvwxyz]{5,})\"\n",
    "    text = re.sub(consonants_5m, \" \", text)\n",
    "    # Remove terms with more than 5 consecutive vowels\n",
    "    vowels_6m = \"([aeiou]{6,})\"\n",
    "    text = re.sub(vowels_6m, \" \", text)\n",
    "    \n",
    "    # Replace non-letter characters with white spaces\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "    # Remove extra white spaces and replace sequences of white spaces with a single white space\n",
    "    text = re.sub('\\s+', ' ', text.strip())\n",
    "    # Remove stop words\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "    \n",
    "# Apply the preprocessing function to df[full_text_col]\n",
    "df[full_text_col] = df[full_text_col].apply(preprocess_text)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Features for - TF Unigram - TF Bigram - TF-IDF Unigram and TF-IDF Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Column full_text (X_text) - Column Relevant (y)\n",
    "\n",
    "le = LabelEncoder()\n",
    "X_text = df['full_text']\n",
    "\n",
    "y = le.fit_transform(df['Relevante'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some items from the dataset\n",
    "#teste_exemplo = df['full_text'][:4]\n",
    "teste_exemplo = X_text[:4]\n",
    "teste_exemplo.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF Unigram\n",
    "tf_vec_unigram = TfidfVectorizer(use_idf=False, norm=\"l1\")\n",
    "X_tf_unigram = tf_vec_unigram.fit_transform(X_text)\n",
    "\n",
    "\n",
    "#TF Bigram\n",
    "tf_vec_bigram = TfidfVectorizer(use_idf=False, norm=\"l1\", ngram_range=(2, 2))\n",
    "X_tf_bigram = tf_vec_bigram.fit_transform(X_text)\n",
    "\n",
    "\n",
    "#TF-IDF Unigram\n",
    "tfidf_vec_unigram = TfidfVectorizer(norm=\"l1\")\n",
    "X_tfidf_unigram = tfidf_vec_unigram.fit_transform(X_text)\n",
    "\n",
    "#TF-IDF Bigram\n",
    "tfidf_vec_bigram = TfidfVectorizer(norm=\"l1\", ngram_range=(2, 2))\n",
    "X_tfidf_bigram = tfidf_vec_bigram.fit_transform(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_tf_unigram)\n",
    "#print(X_tf_bigram)\n",
    "#print(X_tfidf_unigram)\n",
    "#print(X_tfidf_bigram)\n",
    "#X_tfidf_unigram.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo\n",
    "#en_model = KeyedVectors.load_word2vec_format('../wiki.pt.vec')\n",
    "en_model = KeyedVectors.load_word2vec_format('/home/sfilho/wiki.pt.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the tokens \n",
    "words = []\n",
    "for word in en_model.key_to_index:\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[700:710]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# feature (full_text)\n",
    "\n",
    "le = LabelEncoder()\n",
    "X_w_text = df['full_text']\n",
    "\n",
    "y_w = le.fit_transform(df['Relevante'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_w[1]\n",
    "#X_w_text[1]\n",
    "#X_w_ioc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF TRAIN\n",
    "texts = []\n",
    "labels = []\n",
    "for i in range(len(X_w_text)):\n",
    "    texts.append(str(X_w_text[i]))\n",
    "    labels.append(str(y_w[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Relevante.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only the full_text column to create features\n",
    "X_vec = []\n",
    "for t in texts:\n",
    "    vec = []\n",
    "    for d in t.split():\n",
    "        try:\n",
    "            vec.append(en_model.get_vector(d.replace(\"(\",\"\").replace(\")\",\"\")))\n",
    "        except:\n",
    "            vec.append(np.zeros(300))\n",
    "    X_vec.append(np.mean(vec, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = le.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split - Word2vec\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_vec, labels, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train_w2v))\n",
    "print(len(X_test_w2v))\n",
    "print(len(y_train_w2v))\n",
    "print(len(y_test_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_w2v[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = pd.DataFrame(X_train_w2v)\n",
    "X_test_w2v = pd.DataFrame(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels[320:340]\n",
    "#X_vec[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train/Test Split - TF Unigram - TF Bigram - TF-IDF Unigram - TF-IDF Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split - TF Unigram - TF Bigram - TF-IDF Unigram - TF-IDF Bigram\n",
    "\n",
    "X_train_tf_unigram, X_test_tf_unigram, y_train_tf_unigram, y_test_tf_unigram = train_test_split(X_tf_unigram, y, test_size=.2)\n",
    "X_train_tf_bigram, X_test_tf_bigram, y_train_tf_bigram, y_test_tf_bigram = train_test_split(X_tf_bigram, y, test_size=.2)\n",
    "X_train_tfidf_unigram, X_test_tfidf_unigram, y_train_tfidf_unigram, y_test_tfidf_unigram = train_test_split(X_tfidf_unigram, y, test_size=.2)\n",
    "X_train_tfidf_bigram, X_test_tfidf_bigram, y_train_tfidf_bigram, y_test_tfidf_bigram = train_test_split(X_tfidf_bigram, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TF Unigram - Treino:\", X_train_tf_unigram.shape[0], \"Teste:\", X_test_tf_unigram.shape[0])\n",
    "print(\"TF Bigram - Treino:\", X_train_tf_bigram.shape[0], \"Teste:\", X_test_tf_bigram.shape[0])\n",
    "print(\"TF-IDF Unigram - Treino:\", X_train_tfidf_unigram.shape[0], \"Teste:\", X_test_tfidf_unigram.shape[0])\n",
    "print(\"TF-IDF Bigram - Treino:\", X_train_tfidf_bigram.shape[0], \"Teste:\", X_test_tfidf_bigram.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes_tf_unigram = np.bincount(y_train_tf_unigram)\n",
    "test_classes_tf_unigram = np.bincount(y_test_tf_unigram)\n",
    "\n",
    "train_classes_tf_bigram = np.bincount(y_train_tf_bigram)\n",
    "test_classes_tf_bigram = np.bincount(y_test_tf_bigram)\n",
    "\n",
    "train_classes_tfidf_unigram = np.bincount(y_train_tfidf_unigram)\n",
    "test_classes_tfidf_unigram = np.bincount(y_test_tfidf_unigram)\n",
    "\n",
    "train_classes_tfidf_bigram = np.bincount(y_train_tfidf_bigram)\n",
    "test_classes_tfidf_bigram = np.bincount(y_test_tfidf_bigram)\n",
    "\n",
    "print(\"TF Unigram - Treino:\", train_classes_tf_unigram)\n",
    "print(\"TF Unigram - Teste:\", test_classes_tf_unigram)\n",
    "\n",
    "print(\"TF Bigram - Treino:\", train_classes_tf_bigram)\n",
    "print(\"TF Bigram - Teste:\", test_classes_tf_bigram)\n",
    "\n",
    "print(\"TF-IDF Unigram - Treino:\", train_classes_tfidf_unigram)\n",
    "print(\"TF-IDF Unigram - Teste:\", test_classes_tfidf_unigram)\n",
    "\n",
    "print(\"TF-IDF Bigram - Treino:\", train_classes_tfidf_bigram)\n",
    "print(\"TF-IDF Bigram - Teste:\", test_classes_tfidf_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USE TQDM - SVM\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000]}\n",
    " ]\n",
    "svc = LinearSVC(max_iter=1200000)\n",
    "\n",
    "\n",
    "# Creating a decorator to wrap each GridSearchCV with tqdm\n",
    "def grid_search_with_tqdm(clf, X, y):\n",
    "    with tqdm(total=len(param_grid), desc=\"GridSearchCV\") as pbar:\n",
    "        clf = GridSearchCV(clf, param_grid)\n",
    "        clf.fit(X, y)\n",
    "        pbar.update(1)\n",
    "    return clf\n",
    "\n",
    "\n",
    "# Wrapping each GridSearchCV object with tqdm to monitor progress\n",
    "clf_svm_tf_unigram = grid_search_with_tqdm(svc, X_train_tf_unigram, y_train_tf_unigram)\n",
    "clf_svm_tf_bigram = grid_search_with_tqdm(svc, X_train_tf_bigram, y_train_tf_bigram)\n",
    "clf_svm_tfidf_unigram = grid_search_with_tqdm(svc, X_train_tfidf_unigram, y_train_tfidf_unigram)\n",
    "clf_svm_tfidf_bigram = grid_search_with_tqdm(svc, X_train_tfidf_bigram, y_train_tfidf_bigram)\n",
    "clf_svm_w2v = grid_search_with_tqdm(svc, X_train_w2v, y_train_w2v)\n",
    "######\n",
    "\n",
    "svm_y_pred_tf_unigram = clf_svm_tf_unigram.predict(X_test_tf_unigram)\n",
    "svm_y_pred_tf_bigram = clf_svm_tf_bigram.predict(X_test_tf_bigram)\n",
    "svm_y_pred_tfidf_unigram = clf_svm_tfidf_unigram.predict(X_test_tfidf_unigram)\n",
    "svm_y_pred_tfidf_bigram = clf_svm_tfidf_bigram.predict(X_test_tfidf_bigram)\n",
    "svm_y_pred_w2v = clf_svm_w2v.predict(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_tf_unigram, svm_y_pred_tf_unigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('SVM TF Unigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('SVM_TF_Unigram.png', dpi=500)\n",
    "print(classification_report(y_test_tf_unigram, svm_y_pred_tf_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_tf_bigram, svm_y_pred_tf_bigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('SVM TF Bigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('SVM_TF_Bigram.png', dpi=500)\n",
    "print(classification_report(y_test_tf_bigram, svm_y_pred_tf_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_tfidf_unigram, svm_y_pred_tfidf_unigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('SVM TF-IDF Unigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('SVM_TF-IDF_Unigram.png', dpi=500)\n",
    "print(classification_report(y_test_tfidf_unigram, svm_y_pred_tfidf_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_tfidf_bigram, svm_y_pred_tfidf_bigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('SVM TF-IDF Bigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('SVM_TF-IDF_Bigram.png', dpi=500)\n",
    "print(classification_report(y_test_tfidf_bigram, svm_y_pred_tfidf_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_w2v, svm_y_pred_w2v)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('SVM W2V')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('SVM_W2V.png', dpi=500)\n",
    "print(classification_report(y_test_w2v, svm_y_pred_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "clf_rf_tf_unigram = RandomForestClassifier(n_estimators=100, random_state=42).fit(X_train_tf_unigram, y_train_tf_unigram)\n",
    "clf_rf_tf_bigram = RandomForestClassifier(n_estimators=100, random_state=42).fit(X_train_tf_bigram, y_train_tf_bigram)\n",
    "clf_rf_tfidf_unigram = RandomForestClassifier(n_estimators=100, random_state=42).fit(X_train_tfidf_unigram, y_train_tfidf_unigram)\n",
    "clf_rf_tfidf_bigram = RandomForestClassifier(n_estimators=100, random_state=42).fit(X_train_tfidf_bigram, y_train_tfidf_bigram)\n",
    "clf_rf_w2v = RandomForestClassifier(n_estimators=100, random_state=42).fit(X_train_w2v, y_train_w2v)\n",
    "\n",
    "rf_y_pred_tf_unigram = clf_rf_tf_unigram.predict(X_test_tf_unigram)\n",
    "rf_y_pred_tf_bigram = clf_rf_tf_bigram.predict(X_test_tf_bigram)\n",
    "rf_y_pred_tfidf_unigram = clf_rf_tfidf_unigram.predict(X_test_tfidf_unigram)\n",
    "rf_y_pred_tfidf_bigram = clf_rf_tfidf_bigram.predict(X_test_tfidf_bigram)\n",
    "rf_y_pred_w2v = clf_rf_w2v.predict(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_tf_unigram, rf_y_pred_tf_unigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('RF TF Unigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('RF_TF_Unigram.png', dpi=500)\n",
    "print(classification_report(y_test_tf_unigram, rf_y_pred_tf_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_tf_bigram, rf_y_pred_tf_bigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('RF TF Bigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('RF_TF_Bigram.png', dpi=500)\n",
    "print(classification_report(y_test_tf_bigram, rf_y_pred_tf_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_tfidf_unigram, rf_y_pred_tfidf_unigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('RF TF-IDF Unigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('RF_TF-IDF_Unigram.png', dpi=500)\n",
    "print(classification_report(y_test_tfidf_unigram, rf_y_pred_tfidf_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_tfidf_bigram, rf_y_pred_tfidf_bigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('RF TF-IDF Bigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('RF_TF-IDF_Bigram.png', dpi=500)\n",
    "print(classification_report(y_test_tfidf_bigram, rf_y_pred_tfidf_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_w2v, rf_y_pred_w2v)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('RF W2V')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('RF_W2V.png', dpi=500)\n",
    "print(classification_report(y_test_w2v, rf_y_pred_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "clf_lr_tf_unigram = LogisticRegression(C=1000,max_iter=1000,random_state=42).fit(X_train_tf_unigram, y_train_tf_unigram)\n",
    "clf_lr_tf_bigram = LogisticRegression(C=1000,max_iter=1000,random_state=42).fit(X_train_tf_bigram, y_train_tf_bigram)\n",
    "clf_lr_tfidf_unigram = LogisticRegression(C=1000,max_iter=1000,random_state=42).fit(X_train_tfidf_unigram, y_train_tfidf_unigram)\n",
    "clf_lr_tfidf_bigram = LogisticRegression(C=1000,max_iter=1000,random_state=42).fit(X_train_tfidf_bigram, y_train_tfidf_bigram)\n",
    "clf_lr_w2v = LogisticRegression(C=1000,max_iter=1000,random_state=42).fit(X_train_w2v, y_train_w2v)\n",
    "\n",
    "lr_y_pred_tf_unigram = clf_lr_tf_unigram.predict(X_test_tf_unigram)\n",
    "lr_y_pred_tf_bigram = clf_lr_tf_bigram.predict(X_test_tf_bigram)\n",
    "lr_y_pred_tfidf_unigram = clf_lr_tfidf_unigram.predict(X_test_tfidf_unigram)\n",
    "lr_y_pred_tfidf_bigram = clf_lr_tfidf_bigram.predict(X_test_tfidf_bigram)\n",
    "lr_y_pred_w2v = clf_lr_w2v.predict(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_tf_unigram, lr_y_pred_tf_unigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('LR TF Unigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('LR_TF_Unigram.png', dpi=500)\n",
    "print(classification_report(y_test_tf_unigram, lr_y_pred_tf_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_tf_bigram, lr_y_pred_tf_bigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('LR TF Bigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('LR_TF_Bigram.png', dpi=500)\n",
    "print(classification_report(y_test_tf_bigram, lr_y_pred_tf_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_tfidf_unigram, lr_y_pred_tfidf_unigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('LR TF-IDF Unigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('LR_TF-IDF_Unigram.png', dpi=500)\n",
    "print(classification_report(y_test_tfidf_unigram, lr_y_pred_tfidf_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_tfidf_bigram, lr_y_pred_tfidf_bigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('LR TF-IDF Bigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('LR_TF-IDF_Bigram.png', dpi=500)\n",
    "print(classification_report(y_test_tfidf_bigram, lr_y_pred_tfidf_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_w2v, lr_y_pred_w2v)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('LR W2V')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('LR_W2V.png', dpi=500)\n",
    "print(classification_report(y_test_w2v, lr_y_pred_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9\n",
    "}\n",
    "\n",
    "## convert data to float64\n",
    "lgb_X_train_tf_unigram = X_train_tf_unigram.astype(np.float64)\n",
    "lgb_y_train_tf_unigram = y_train_tf_unigram.astype(np.float64)\n",
    "lgb_X_train_tf_bigram = X_train_tf_bigram.astype(np.float64)\n",
    "lgb_y_train_tf_bigram = y_train_tf_bigram.astype(np.float64)\n",
    "lgb_X_train_tfidf_unigram = X_train_tfidf_unigram.astype(np.float64)\n",
    "lgb_y_train_tfidf_unigram = y_train_tfidf_unigram.astype(np.float64)\n",
    "lgb_X_train_tfidf_bigram = X_train_tfidf_bigram.astype(np.float64)\n",
    "lgb_y_train_tfidf_bigram = y_train_tfidf_bigram.astype(np.float64)\n",
    "##\n",
    "lgb_X_train_w2v = X_train_w2v\n",
    "lgb_X_test_w2v  = X_test_w2v\n",
    "##\n",
    "\n",
    "\n",
    "## convert data to float64\n",
    "lgb_X_test_tf_unigram = X_test_tf_unigram.astype(np.float64)\n",
    "lgb_y_test_tf_unigram = y_test_tf_unigram.astype(np.float64)\n",
    "lgb_X_test_tf_bigram = X_test_tf_bigram.astype(np.float64)\n",
    "lgb_y_test_tf_bigram = y_test_tf_bigram.astype(np.float64)\n",
    "lgb_X_test_tfidf_unigram = X_test_tfidf_unigram.astype(np.float64)\n",
    "lgb_y_test_tfidf_unigram = y_test_tfidf_unigram.astype(np.float64)\n",
    "lgb_X_test_tfidf_bigram = X_test_tfidf_bigram.astype(np.float64)\n",
    "lgb_y_test_tfidf_bigram = y_test_tfidf_bigram.astype(np.float64)\n",
    "##\n",
    "lgb_y_test_w2v = y_test_w2v\n",
    "lgb_y_train_w2v = y_train_w2v\n",
    "####\n",
    "##\n",
    "\n",
    "train_data = lgb.Dataset(lgb_X_train_tf_unigram, label=lgb_y_train_tf_unigram)\n",
    "clf_lgb_tf_unigram = lgb.train(params, train_data, 100)\n",
    "\n",
    "train_data = lgb.Dataset(lgb_X_train_tf_bigram, label=lgb_y_train_tf_bigram)\n",
    "clf_lgb_tf_bigram = lgb.train(params, train_data, 100)\n",
    "\n",
    "train_data = lgb.Dataset(lgb_X_train_tfidf_unigram, label=lgb_y_train_tfidf_unigram)\n",
    "clf_lgb_tfidf_unigram = lgb.train(params, train_data, 100)\n",
    "\n",
    "train_data = lgb.Dataset(lgb_X_train_tfidf_bigram, label=lgb_y_train_tfidf_bigram)\n",
    "clf_lgb_tfidf_bigram = lgb.train(params, train_data, 100)\n",
    "\n",
    "train_data = lgb.Dataset(lgb_X_train_w2v, label=lgb_y_train_w2v)\n",
    "clf_lgb_w2v = lgb.train(params, train_data, 100)\n",
    "\n",
    "\n",
    "lgb_y_pred_tf_unigram = clf_lgb_tf_unigram.predict(lgb_X_test_tf_unigram)\n",
    "lgb_y_pred_tf_bigram = clf_lgb_tf_bigram.predict(lgb_X_test_tf_bigram)\n",
    "lgb_y_pred_tfidf_unigram = clf_lgb_tfidf_unigram.predict(lgb_X_test_tfidf_unigram)\n",
    "lgb_y_pred_tfidf_bigram = clf_lgb_tfidf_bigram.predict(lgb_X_test_tfidf_bigram)\n",
    "lgb_y_pred_w2v = clf_lgb_w2v.predict(lgb_X_test_w2v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_y_pred_tf_unigram = np.round(lgb_y_pred_tf_unigram).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(lgb_y_test_tf_unigram, lgb_y_pred_tf_unigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('LGB TF Unigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('LGB_TF_Unigram.png', dpi=500)\n",
    "print(classification_report(lgb_y_test_tf_unigram, lgb_y_pred_tf_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_y_pred_tf_bigram = np.round(lgb_y_pred_tf_bigram).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(lgb_y_test_tf_bigram, lgb_y_pred_tf_bigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('LGB TF Bigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('LGB_TF_Bigram.png', dpi=500)\n",
    "print(classification_report(lgb_y_test_tf_bigram, lgb_y_pred_tf_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_y_pred_tfidf_unigram = np.round(lgb_y_pred_tfidf_unigram).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(lgb_y_test_tfidf_unigram, lgb_y_pred_tfidf_unigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('LGB TF-IDF Unigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('LGB_TF-IDF_Unigram.png', dpi=500)\n",
    "print(classification_report(lgb_y_test_tfidf_unigram, lgb_y_pred_tfidf_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_y_pred_tfidf_bigram = np.round(lgb_y_pred_tfidf_bigram).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(lgb_y_test_tfidf_bigram, lgb_y_pred_tfidf_bigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('LGB TF-IDF Bigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('LGB_TF-IDF_Bigram.png', dpi=500)\n",
    "print(classification_report(lgb_y_test_tfidf_bigram, lgb_y_pred_tfidf_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_y_pred_w2v = np.round(lgb_y_pred_w2v).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(lgb_y_test_w2v, lgb_y_pred_w2v)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('LGB W2V')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('LGB_W2V.png', dpi=500)\n",
    "print(classification_report(lgb_y_test_w2v, lgb_y_pred_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "clf_xgb_tf_unigram = XGBClassifier(max_depth=9, \n",
    "                    learning_rate=0.1,\n",
    "                    n_estimators=2245, \n",
    "                    silent=True, \n",
    "                    nthread=-1,\n",
    "                    missing=np.nan, \n",
    "                    objective='binary:logistic',\n",
    "                    gamma=0.0, \n",
    "                    min_child_weight=1, \n",
    "                    max_delta_step=2, \n",
    "                    subsample=0.39, \n",
    "                    colsample_bytree=0.53, \n",
    "                    base_score=0.5, \n",
    "                    seed=395277)\n",
    "\n",
    "clf_xgb_tf_bigram = XGBClassifier(max_depth=9, \n",
    "                    learning_rate=0.1,\n",
    "                    n_estimators=2245, \n",
    "                    silent=True, \n",
    "                    nthread=-1,\n",
    "                    missing=np.nan, \n",
    "                    objective='binary:logistic',\n",
    "                    gamma=0.0, \n",
    "                    min_child_weight=1, \n",
    "                    max_delta_step=2, \n",
    "                    subsample=0.39, \n",
    "                    colsample_bytree=0.53, \n",
    "                    base_score=0.5, \n",
    "                    seed=395277)\n",
    "\n",
    "clf_xgb_tfidf_unigram = XGBClassifier(max_depth=9, \n",
    "                    learning_rate=0.1,\n",
    "                    n_estimators=2245, \n",
    "                    silent=True, \n",
    "                    nthread=-1,\n",
    "                    missing=np.nan, \n",
    "                    objective='binary:logistic',\n",
    "                    gamma=0.0, \n",
    "                    min_child_weight=1, \n",
    "                    max_delta_step=2, \n",
    "                    subsample=0.39, \n",
    "                    colsample_bytree=0.53, \n",
    "                    base_score=0.5, \n",
    "                    seed=395277)\n",
    "\n",
    "clf_xgb_tfidf_bigram = XGBClassifier(max_depth=9, \n",
    "                    learning_rate=0.1,\n",
    "                    n_estimators=2245, \n",
    "                    silent=True, \n",
    "                    nthread=-1,\n",
    "                    missing=np.nan, \n",
    "                    objective='binary:logistic',\n",
    "                    gamma=0.0, \n",
    "                    min_child_weight=1, \n",
    "                    max_delta_step=2, \n",
    "                    subsample=0.39, \n",
    "                    colsample_bytree=0.53, \n",
    "                    base_score=0.5, \n",
    "                    seed=395277)\n",
    "\n",
    "clf_xgb_w2v = XGBClassifier(max_depth=9, \n",
    "                    learning_rate=0.1,\n",
    "                    n_estimators=2245, \n",
    "                    silent=True, \n",
    "                    nthread=-1,\n",
    "                    missing=np.nan, \n",
    "                    objective='binary:logistic',\n",
    "                    gamma=0.0, \n",
    "                    min_child_weight=1, \n",
    "                    max_delta_step=2, \n",
    "                    subsample=0.39, \n",
    "                    colsample_bytree=0.53, \n",
    "                    base_score=0.5, \n",
    "                    seed=395277)\n",
    "###\n",
    "\n",
    "clf_xgb_tf_unigram.fit(X_train_tf_unigram, y_train_tf_unigram)\n",
    "clf_xgb_tf_bigram.fit(X_train_tf_bigram, y_train_tf_bigram)\n",
    "clf_xgb_tfidf_unigram.fit(X_train_tfidf_unigram, y_train_tfidf_unigram)\n",
    "clf_xgb_tfidf_bigram.fit(X_train_tfidf_bigram, y_train_tfidf_bigram)\n",
    "clf_xgb_w2v.fit(X_train_w2v, y_train_w2v)\n",
    "\n",
    "\n",
    "xgb_y_pred_tf_unigram = clf_xgb_tf_unigram.predict(X_test_tf_unigram)\n",
    "xgb_y_pred_tf_bigram = clf_xgb_tf_bigram.predict(X_test_tf_bigram)\n",
    "xgb_y_pred_tfidf_unigram = clf_xgb_tfidf_unigram.predict(X_test_tfidf_unigram)\n",
    "xgb_y_pred_tfidf_bigram = clf_xgb_tfidf_bigram.predict(X_test_tfidf_bigram)\n",
    "xgb_y_pred_w2v = clf_xgb_w2v.predict(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_tf_unigram, xgb_y_pred_tf_unigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('XGB TF Unigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('XGB_TF_Unigram.png', dpi=500)\n",
    "print(classification_report(y_test_tf_unigram, xgb_y_pred_tf_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_tf_bigram, xgb_y_pred_tf_bigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('XGB TF Bigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('XGB_TF_Bigram.png', dpi=500)\n",
    "print(classification_report(y_test_tf_bigram, xgb_y_pred_tf_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_tfidf_unigram, xgb_y_pred_tfidf_unigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('XGB TF-IDF Unigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('XGB_TF-IDF_Unigram.png', dpi=500)\n",
    "print(classification_report(y_test_tfidf_unigram, xgb_y_pred_tfidf_unigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_tfidf_bigram, xgb_y_pred_tfidf_bigram)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('XGB TF-IDF Bigram')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('XGB_TF-IDF_Bigram.png', dpi=500)\n",
    "print(classification_report(y_test_tfidf_bigram, xgb_y_pred_tfidf_bigram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['NÃO', 'SIM']\n",
    "cf_matrix = metrics.confusion_matrix(y_test_w2v, xgb_y_pred_w2v)\n",
    "ax = sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')\n",
    "ax.set_title('XGB W2V')\n",
    "ax.set_xlabel('Valores previstos')\n",
    "ax.set_ylabel('Valores reais')\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "figure = ax.get_figure()\n",
    "figure.savefig('XGB_W2V.png', dpi=500)\n",
    "print(classification_report(y_test_w2v, xgb_y_pred_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save the Best Model for Use in New Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the best-trained model to test on new data\n",
    "with open('modelo_lgb_tfidf_unigram_ptbr.pkl', 'wb') as file:\n",
    "    pickle.dump(clf_lgb_tfidf_unigram, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF-IDF vectorizer to a file using pickle\n",
    "with open('tfidf_vec_unigram_ptbr.pkl', 'wb') as file:\n",
    "    pickle.dump(tfidf_vec_unigram, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
