{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7618d7de-3621-4d6e-9551-abab51fcbcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b53fca-9b83-4ec6-9911-4563ba046600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "\n",
    "# Dataset II\n",
    "#df = pd.read_csv('./dataset_II.csv')\n",
    "\n",
    "# Select only Non-Relevant posts\n",
    "#df = df[df['Relevante'] == 'Não'].reset_index(drop=True)\n",
    "\n",
    "# Select only Relevant posts\n",
    "#df = df[df['Relevante'] == 'Sim'].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Dataset III\n",
    "df = pd.read_csv('./dataset_III.csv')\n",
    "\n",
    "# Select only Non-Relevant posts\n",
    "#df = df[df['previsao_binaria'] == 0].reset_index(drop=True)\n",
    "\n",
    "# Select only Relevant posts\n",
    "df = df[df['previsao_binaria'] == 1].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Select only posts predicted as Low Relevance\n",
    "#df = df[df['Relevância'] == 'Baixa'].reset_index(drop=True)\n",
    "\n",
    "# Select only posts predicted as Medium Relevance\n",
    "#df = df[df['Relevância'] == 'Média'].reset_index(drop=True)\n",
    "\n",
    "# Select only posts predicted as High Relevanc\n",
    "#df = df[df['Relevância'] == 'Alta'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b95d70-62c8-4bcd-9070-9cc347a25ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434bb8cd-864e-418e-badc-f7a62469d663",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Remover novas stopwors\n",
    "stop_words = [\"aqui\", \"pode\", \"sobre\", \"fazer\", \"alguem\", \"tudo\", \"coisa\", \"novato\", \"bem\",\n",
    "              \"vou\", \"sei\", \"boca\", \"algum\", \"alguns\", \"alguma\", \"algo\", \"nada\", \"bom\", \"entao\",\n",
    "              \"quer\", \"the\", \"and\", \"you\", \"cara\", \"coisas\", \"sim\", \"ainda\", \"ver\", \"usar\", \n",
    "              \"assim\", \"acho\"]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0bd918-d388-4c39-a3ea-c74f073ea86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def preprocess_text_b(text):\n",
    "    # Separa em tokens\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [word for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# Aplica a função preprocess_text_b a cada documento e cria uma lista de listas de tokens\n",
    "documents = [preprocess_text_b(doc) for doc in df['text']]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ea419-0af7-44e2-bf51-4079d6bfc356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the column full_text\n",
    "documents = df['full_text']\n",
    "\n",
    "\n",
    "# Transform the list of documents into a list of lists of words\n",
    "texts = [str(doc).split() for doc in documents]\n",
    "\n",
    "\n",
    "# Create a dictionary with the words\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Filter out rare words and stopwords\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "# Create the corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Create the LDA model with 10 topics\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    num_topics=10,\n",
    "    id2word=dictionary,\n",
    "    random_state=51,\n",
    "    passes=10,\n",
    "    iterations=1000,\n",
    "    minimum_probability=0.01\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "# Create lists to store the indices of documents\n",
    "document_indexes = [[] for _ in range(lda_model.num_topics)]\n",
    "\n",
    "# Iterate through the topics\n",
    "for i in range(lda_model.num_topics):\n",
    "    # Select terms from the topic with a minimum probability of 0.01\n",
    "    topic_terms = lda_model.get_topic_terms(i)\n",
    "    # Print the selected terms\n",
    "    print(f\"Tópico {i}: {[dictionary[id] for id, _ in topic_terms]}\")\n",
    "    \n",
    "   \n",
    "    # Retrieve documents with the highest probability for the current topic\n",
    "    top_documents = []\n",
    "    for doc in corpus:\n",
    "        doc_topics = lda_model.get_document_topics(doc, minimum_probability=0)\n",
    "        if doc_topics:\n",
    "            top_topic = max(doc_topics, key=lambda x: x[1])[0]\n",
    "            if top_topic == i:\n",
    "                top_documents.append(doc)\n",
    "    \n",
    "    # Store the indices of documents for the current topic    \n",
    "    document_indexes[i] = [i for i, doc in enumerate(corpus) if doc in top_documents]\n",
    "    \n",
    "\n",
    "print()  # Blank line for separation\n",
    "\n",
    "# Quantify how many documents are associated with each topic\n",
    "topic_count = [0] * lda_model.num_topics\n",
    "for doc in corpus:\n",
    "    doc_topics = lda_model.get_document_topics(doc, minimum_probability=0)\n",
    "    if doc_topics:\n",
    "        top_topic = max(doc_topics, key=lambda x: x[1])[0]\n",
    "        topic_count[top_topic] += 1\n",
    "\n",
    "total_docs = 0\n",
    "for i, count in enumerate(topic_count):\n",
    "    print(f\"Quantidade de documentos no tópico {i}: {count}\")           \n",
    "    total_docs += count\n",
    "print(f\"Total de Documentos: {total_docs}\")\n",
    "\n",
    "# Print the lists of document indices\n",
    "for i in range(lda_model.num_topics):\n",
    "    print()  # Blank line to separate the topics\n",
    "    print(f\"Documentos do tópico {i}: {document_indexes[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0e37bb-8fb9-4db0-998d-1ee9be8e922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the text to count the word frequencies\n",
    "\n",
    "text = ' '.join(df['full_text'].dropna().tolist())\n",
    "\n",
    "\n",
    "# Split the text into words\n",
    "words = text.split()\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Extract the top 100 most frequent words\n",
    "top_words = word_counts.most_common(100)\n",
    "\n",
    "# Extract the words and their counts\n",
    "word_list, count_list = zip(*top_words)\n",
    "\n",
    "\n",
    "# Create the numbered list of words\n",
    "numbered_word_list = [f\"{i+1} - {word}\" for i, word in enumerate(word_list)]\n",
    "\n",
    "# Display the numbered list on the console\n",
    "for numbered_word in numbered_word_list:\n",
    "    print(numbered_word)\n",
    "\n",
    "# Save the numbered list to a text file\n",
    "with open(\"100_lista_palavras_mais_frequentes_III_rel.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write('\\n'.join(numbered_word_list))\n",
    "\n",
    "\n",
    "# To display the word counts in a bar chart\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.bar(word_list, count_list)\n",
    "plt.xlabel('Palavras (100 mais frequentes)')\n",
    "plt.ylabel('Frequência')\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig(\"100palavras_mais_frequentes_CJ_DADOS_III_relevantes.png\", dpi=500, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
